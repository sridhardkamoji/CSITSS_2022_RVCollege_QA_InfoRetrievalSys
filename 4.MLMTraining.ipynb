{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLM Fine tuning (domain Adaption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://drive.google.com/drive/folders/1Q9prRy02buQ09X5RDia-KMO1G19qPwws?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://huggingface.co/course/chapter7/3?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sridhar Kamoji\\AppData\\Roaming\\Python\\Python38\\site-packages\\huggingface_hub\\snapshot_download.py:6: FutureWarning: snapshot_download.py has been made private and will no longer be available from version 0.11. Please use `from huggingface_hub import snapshot_download` to import the only public function in this module. Other members of the file may be changed without a deprecation notice.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import nltk\n",
    "import sentence_transformers\n",
    "import ast\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, DistilBertConfig\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained Distilbert modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "config = DistilBertConfig(output_hidden_states = True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint, config = config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration fabiochiu--medium-articles-96791ff68926910d\n",
      "Reusing dataset csv (C:\\Users\\Sridhar Kamoji\\.cache\\huggingface\\datasets\\fabiochiu___csv\\fabiochiu--medium-articles-96791ff68926910d\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36804afdbb3c463788b251d959987935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset Ref:\n",
    "# https://huggingface.co/datasets/fabiochiu/medium-articles\n",
    "data = datasets.load_dataset('fabiochiu/medium-articles', data_files= 'medium_articles.csv')\n",
    "\n",
    "# converting the dataset_dictionary object to pandas dataframe\n",
    "data = pd.DataFrame.from_dict(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tags'] = [ast.literal_eval(x) for x in data['tags']]\n",
    "data['tags_joined'] = [', '.join(x) for x in data['tags']]\n",
    "\n",
    "data['bert_text'] = data['title'] + '\\n\\n\\n' + data['text'] + '\\n\\n\\n' + 'This article belong to the following segments ' + data['tags_joined']\n",
    "\n",
    "data['label'] = 0\n",
    "\n",
    "data.dropna(inplace = True)\n",
    "data.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192361, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the data for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = data[['bert_text', 'label']].sample(n = 2000).copy()\n",
    "train_dataset = Dataset.from_dict(train_df)\n",
    "\n",
    "# converting into dataset object\n",
    "bert_dataset = DatasetDict({'train': train_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['bert_text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> Review: My Body — The Frenemy. Trigger Warning: Strong language, body…\n",
      "\n",
      "\n",
      "Trigger Warning: Strong language, body dysmorphia, self harm, disordered eating, anxiety, depression\n",
      "\n",
      "For most of my childhood I had no issues with my body. It looked fine, it worked fine, I didn’t think twice about it.\n",
      "\n",
      "The first time I was forced to consider my physicality was at the end of college when my boyfriend at the time called me pleasantly plump. It was a shock to me, Ms. Effortlessly Skinny that I could ever be perceived as plump. Seeing myself in the mirror I wouldn’t have really considered myself fat. I saw myself as I always saw myself. A few extra stretch marks here and there, but the rest seemed fine.\n",
      "\n",
      "It was only through other people’s eyes that I began to notice that I was fat. A few years later my brother also commented that I looked like a family friend of ours who had very big hips. I still couldn’t really wrap my head around it. But over time I started looking around and comparing with everyone else. Yes, I did have to buy Large pants and not just because I’m tall. I googled a bunch of statistics about ideal weight and did some BMI calculations and realized that I was what they called plus size.\n",
      "\n",
      "What the fuck? How was I plus size? When did this even happen? Why is the bar for plus size so low? At clothing stores I started to notice that some places didn’t even carry my size, or my size was never available. Were clothes only meant for small and medium sized people? What were people my size doing?\n",
      "\n",
      "Still, I was 22 and resolved to fix it. I had never been fat and I refused to believe that I had to remain fat. I tried a million different diets. Low carb, keto, calorie counting. I tried weight lifting, yoga, zumba, cardio, whatever was accessible. Then two years later, at 24, I finally found the perfect combination. Powerlifting and low carb. In three months I lost most of the weight. The rest came off slower but by age 25 I was looking slender again. WHEW. I had done it. Mission accomplished, story over, happy ending, roll credits.\n",
      "\n",
      "WRONG.\n",
      "\n",
      "Photo by Anete Lusina from Pexels\n",
      "\n",
      "Age 27. I was asked to gain weight for a film. Cocky from having lost most of my weight in three months, I agreed. I could lose it again. What was the big deal?\n",
      "\n",
      "Age 29. It’s been three years and the weight is still on. I lost and gained and lost weight a few times in between thanks to surgery, international travel, and the death of a loved one, interspersed with rigorous diet, physiotherapy and exercise.\n",
      "\n",
      "I turn 30 in two weeks and I’m in a constant state of conflict. There’s a part of me that wants to just give up, give in, lean into this new body, and own it. A part of me has done that already. The part of me that exercises to feel good, eats clean for clear skin, who carries toddlers and moves furniture and rides a scooter is very happy with my body. I gave away my “thin” clothes, bought new ones in Large, Extra Large, and Extra Extra Large, depending on the brand and the product. I follow body positivity influencers and brands on Instagram, I unfollowed fitness models and actresses with their rail thin bodies. I bought shape wear and high waisted trousers and I’m living my best life. This part of me wears my weight with a middle finger in the air saying “fuck you and you expectations of what my body should look like.”\n",
      "\n",
      "But the other part of me is unwilling to accept this turn of events. Absolutely egotistically fixated on chasing my “ideal” weight. To be completely transparent, I’m only 2 kgs above the ideal maximum for my height. I’m at 77 kgs, 75 is the upper limit of ideal for my height, and my goal weight is 60 kgs. The lower limit for my height is 55 kgs, which would be the REAL ideal. But I’ve hit 60 kgs before and for now I would like to at least get back there. I’ve done it before and I can do it again. 77 now, 75 soon, 70 in a few months, 60 next year, and 55 a few months later. Right?\n",
      "\n",
      "Wrong. What I can’t seem to factor into my calculations is my mental health. At 24 I was in prime shape physically, but mentally I was at my worst. Severely, deeply depressed — crying multiple times a day, every day and hoping and waiting for death to come. I hated being alive. Chasing this damn goal weight was the only thing that gave me a sense of purpose and control.\n",
      "\n",
      "I was motivated purely by self loathing. Every rep in the gym was fuelled by “Move your disgusting body you piece of shit”. Every skipped dinner came from “You don’t deserve food. You deserve to be hungry. Burn your fat, you sick fuck.” On cheat days I would binge eat and vomit because my body couldn’t take what my brain so desperately wanted.\n",
      "\n",
      "Even at 27, just before I gained the weight for the film, I was all over the placed. Recently diagnosed with multiple disorders, emotionally unstable, re-evaluating the foundation on which I had built my life, self harming, anxious, and turbulent. But physically, I was at that sweet 60 kgs on the scale. I barely worked out, lost most of my muscle mass, couldn’t handle my motorcycle, but hey, I was buying Small tops and Medium pants, so all good eh?\n",
      "\n",
      "As 30 comes up, I am emotionally much more aware. Four years of therapy, regular medication, a hundred tools and worksheets and journal entries and meditation sessions later, I am at least aware of my patterns. I don’t fall apart as easily, I don’t get as angry, I am no longer numb and suicidal. I see the joy in the little things — a smile, a hug, a day at the beach, a piece of chocolate. It’s all so much more fulfilling now that my brain gets enough serotonin.\n",
      "\n",
      "The better my mental health gets, the less I seem to be able to punish my body into thinness. Was thinness merely a side effect of a turbulent mind? Was my body fat percentage the only thing I could control while I was a slave to the demons in my head? Was losing weight the only way I could feel a sense of self worth? Is it such a bad thing to be happy and fat?\n",
      "\n",
      "Strangely, regardless of how I feel, it appears that my fatness bothers people so much more than my mental illness did. A thin, mentally ill person doesn’t raise as many eyebrows as a happy fat person. “How strange to be happy when you’re fat”, their looks say. I too am no longer able to fuel my workouts with self loathing. I am no longer able to skip dinner as often as I used to. Every now and then, that voice resurfaces. When I have to send my measurements to a stylist or take pictures in a swimsuit, I suddenly become aware of my dimensions again. Thankfully those moments are only moments now. At the most a few hours on a bad day. No longer my status quo, my default setting.\n",
      "\n",
      "As 30 comes up, I am fighting hard to silence that cruel voice that only has eyes for me. The cruel voice is quiet when I see other people’s bodies, when I work out, when I eat clean. It only rises when I stand on the scale, or measure myself, or try on clothes. As 30 comes up, I will fight harder to silence that voice. I will lean in to the part of me that is happy, that is active, that is mindful about eating. I will fall apart once a month or so, but there will be no more skipped dinners and binge/purge cheat days. There will instead be an hour of physical activity every day, a little cheese on my bread, a little chocolate on my tongue, and a bit of beer on a Sunday.\n",
      "\n",
      "Whatever the scale will be, will be.\n",
      "\n",
      "\n",
      "This article belong to the following segments Body Positivity, Weight Loss, Weight, Body Acceptance, Acceptance'\n",
      "'>>> Label: My Body — The Frenemy. Trigger Warning: Strong language, body…\n",
      "\n",
      "\n",
      "Trigger Warning: Strong language, body dysmorphia, self harm, disordered eating, anxiety, depression\n",
      "\n",
      "For most of my childhood I had no issues with my body. It looked fine, it worked fine, I didn’t think twice about it.\n",
      "\n",
      "The first time I was forced to consider my physicality was at the end of college when my boyfriend at the time called me pleasantly plump. It was a shock to me, Ms. Effortlessly Skinny that I could ever be perceived as plump. Seeing myself in the mirror I wouldn’t have really considered myself fat. I saw myself as I always saw myself. A few extra stretch marks here and there, but the rest seemed fine.\n",
      "\n",
      "It was only through other people’s eyes that I began to notice that I was fat. A few years later my brother also commented that I looked like a family friend of ours who had very big hips. I still couldn’t really wrap my head around it. But over time I started looking around and comparing with everyone else. Yes, I did have to buy Large pants and not just because I’m tall. I googled a bunch of statistics about ideal weight and did some BMI calculations and realized that I was what they called plus size.\n",
      "\n",
      "What the fuck? How was I plus size? When did this even happen? Why is the bar for plus size so low? At clothing stores I started to notice that some places didn’t even carry my size, or my size was never available. Were clothes only meant for small and medium sized people? What were people my size doing?\n",
      "\n",
      "Still, I was 22 and resolved to fix it. I had never been fat and I refused to believe that I had to remain fat. I tried a million different diets. Low carb, keto, calorie counting. I tried weight lifting, yoga, zumba, cardio, whatever was accessible. Then two years later, at 24, I finally found the perfect combination. Powerlifting and low carb. In three months I lost most of the weight. The rest came off slower but by age 25 I was looking slender again. WHEW. I had done it. Mission accomplished, story over, happy ending, roll credits.\n",
      "\n",
      "WRONG.\n",
      "\n",
      "Photo by Anete Lusina from Pexels\n",
      "\n",
      "Age 27. I was asked to gain weight for a film. Cocky from having lost most of my weight in three months, I agreed. I could lose it again. What was the big deal?\n",
      "\n",
      "Age 29. It’s been three years and the weight is still on. I lost and gained and lost weight a few times in between thanks to surgery, international travel, and the death of a loved one, interspersed with rigorous diet, physiotherapy and exercise.\n",
      "\n",
      "I turn 30 in two weeks and I’m in a constant state of conflict. There’s a part of me that wants to just give up, give in, lean into this new body, and own it. A part of me has done that already. The part of me that exercises to feel good, eats clean for clear skin, who carries toddlers and moves furniture and rides a scooter is very happy with my body. I gave away my “thin” clothes, bought new ones in Large, Extra Large, and Extra Extra Large, depending on the brand and the product. I follow body positivity influencers and brands on Instagram, I unfollowed fitness models and actresses with their rail thin bodies. I bought shape wear and high waisted trousers and I’m living my best life. This part of me wears my weight with a middle finger in the air saying “fuck you and you expectations of what my body should look like.”\n",
      "\n",
      "But the other part of me is unwilling to accept this turn of events. Absolutely egotistically fixated on chasing my “ideal” weight. To be completely transparent, I’m only 2 kgs above the ideal maximum for my height. I’m at 77 kgs, 75 is the upper limit of ideal for my height, and my goal weight is 60 kgs. The lower limit for my height is 55 kgs, which would be the REAL ideal. But I’ve hit 60 kgs before and for now I would like to at least get back there. I’ve done it before and I can do it again. 77 now, 75 soon, 70 in a few months, 60 next year, and 55 a few months later. Right?\n",
      "\n",
      "Wrong. What I can’t seem to factor into my calculations is my mental health. At 24 I was in prime shape physically, but mentally I was at my worst. Severely, deeply depressed — crying multiple times a day, every day and hoping and waiting for death to come. I hated being alive. Chasing this damn goal weight was the only thing that gave me a sense of purpose and control.\n",
      "\n",
      "I was motivated purely by self loathing. Every rep in the gym was fuelled by “Move your disgusting body you piece of shit”. Every skipped dinner came from “You don’t deserve food. You deserve to be hungry. Burn your fat, you sick fuck.” On cheat days I would binge eat and vomit because my body couldn’t take what my brain so desperately wanted.\n",
      "\n",
      "Even at 27, just before I gained the weight for the film, I was all over the placed. Recently diagnosed with multiple disorders, emotionally unstable, re-evaluating the foundation on which I had built my life, self harming, anxious, and turbulent. But physically, I was at that sweet 60 kgs on the scale. I barely worked out, lost most of my muscle mass, couldn’t handle my motorcycle, but hey, I was buying Small tops and Medium pants, so all good eh?\n",
      "\n",
      "As 30 comes up, I am emotionally much more aware. Four years of therapy, regular medication, a hundred tools and worksheets and journal entries and meditation sessions later, I am at least aware of my patterns. I don’t fall apart as easily, I don’t get as angry, I am no longer numb and suicidal. I see the joy in the little things — a smile, a hug, a day at the beach, a piece of chocolate. It’s all so much more fulfilling now that my brain gets enough serotonin.\n",
      "\n",
      "The better my mental health gets, the less I seem to be able to punish my body into thinness. Was thinness merely a side effect of a turbulent mind? Was my body fat percentage the only thing I could control while I was a slave to the demons in my head? Was losing weight the only way I could feel a sense of self worth? Is it such a bad thing to be happy and fat?\n",
      "\n",
      "Strangely, regardless of how I feel, it appears that my fatness bothers people so much more than my mental illness did. A thin, mentally ill person doesn’t raise as many eyebrows as a happy fat person. “How strange to be happy when you’re fat”, their looks say. I too am no longer able to fuel my workouts with self loathing. I am no longer able to skip dinner as often as I used to. Every now and then, that voice resurfaces. When I have to send my measurements to a stylist or take pictures in a swimsuit, I suddenly become aware of my dimensions again. Thankfully those moments are only moments now. At the most a few hours on a bad day. No longer my status quo, my default setting.\n",
      "\n",
      "As 30 comes up, I am fighting hard to silence that cruel voice that only has eyes for me. The cruel voice is quiet when I see other people’s bodies, when I work out, when I eat clean. It only rises when I stand on the scale, or measure myself, or try on clothes. As 30 comes up, I will fight harder to silence that voice. I will lean in to the part of me that is happy, that is active, that is mindful about eating. I will fall apart once a month or so, but there will be no more skipped dinners and binge/purge cheat days. There will instead be an hour of physical activity every day, a little cheese on my bread, a little chocolate on my tongue, and a bit of beer on a Sunday.\n",
      "\n",
      "Whatever the scale will be, will be.\n",
      "\n",
      "\n",
      "This article belong to the following segments Body Positivity, Weight Loss, Weight, Body Acceptance, Acceptance'\n",
      "\n",
      "'>>> Review: If You’re Under 50, You Might Not Need To Save for Retirement\n",
      "\n",
      "\n",
      "The other day my Mother gave me an update on what houses in my parents’ neighborhood have been going for. Based on this rough assessment, the house they live in would likely fetch around $150,000 today.\n",
      "\n",
      "My parents bought their house for $85,000 about 28 years ago.\n",
      "\n",
      "If you picked up their house, slapped on a Victorian facade, and moved it to the San Francisco neighborhood where I spent the last few days, it would go for well over a million dollars. In pretty much any neighborhood in The City, you’re looking at $1,000,000. In most parts of California, you could probably ask and get no less than half a million.\n",
      "\n",
      "Location, location, location!\n",
      "\n",
      "The house my parents live in, along with what I think is a modest amount of cash. That’s my inheritance.\n",
      "\n",
      "I’m probably looking at approximately $200,000 to $250,000 total.\n",
      "\n",
      "Here’s the upside — I have an insanely low cost of living that I expect to continue to go lower.\n",
      "\n",
      "I live in Los Angeles. I spend relatively very little on rent. I have a modest car. As my income continues to increase (fingers crossed!), these expenses look to trend even lower.\n",
      "\n",
      "I’m in a solid situation. I look at $100,000 or $200,000 the way other people — with higher costs of living — might look at a million dollars.\n",
      "\n",
      "This is the one thing we don’t fully comprehend about having a low cost of living.\n",
      "\n",
      "We riff condescendingly about inflation. We platitude about making our money go further. But we don’t consider how a low cost of living changes how we value money and decisions such as whether or not to be in the stock market:\n",
      "\n",
      "If you make the decision to get out of the stock market, it should have very little, if anything, to do with making or losing money. It should have everything to do with a realization that you can meet your needs — and then some — with consistent cash flow and a focus on using money to essentially love yourself and love the ones you’re with.\n",
      "\n",
      "Which leads directly to the touchy subject of expecting or counting on or considering however you choose to consider your inheritance (assuming you’re going to get one).\n",
      "\n",
      "As I detailed in a November 2020, Making of a Millionaire article, young people stand to inherit a considerable amount of cash over the next several decades.\n",
      "\n",
      "Call me a callous tool, but this absolutely figures into my financial planning:\n",
      "\n",
      "…I have a pretty good idea how much money I stand to inherit when my parents pass away. My dad just turned 86. My mom is 73. Here’s hoping the inevitable doesn’t happen for a good 20 or 30 years (knock on wood!). But it’s going to happen. Even if I don’t run around thinking about it every day, the relative certainty of receiving what will probably be somewhere in the low-six figures plays into my long-term financial planning. It has to. I’d be stupid to ignore it.\n",
      "\n",
      "The more I think about it, the more I think if you’re in a situation similar to or better than mine, you’d be really stupid to not factor an inheritance into your financial planning.\n",
      "\n",
      "In fact, if you’re under 50, it could be one of the cornerstones of your personal financial situation as you enter relative old age. It’s another reason why young people have the modern luxury of eschewing the notion of traditional retirement.\n",
      "\n",
      "In other words, you don’t have to save for retirement. You save for living.\n",
      "\n",
      "This is not to say you run reckless with your money because you know you’re getting a windfall at some still unknown date in the future. That would be really, really stupid.\n",
      "\n",
      "However, it is to say it’s perfectly okay to acknowledge that the money you’ll likely receive can, probably will, and definitely should have a material impact on your life and future, even if it simply means it lends a hand in facilitating the future you envision.\n",
      "\n",
      "You get a lot of backlash when you talk like this. When you factor in the money your parents or some other relative or friend will leave you.\n",
      "\n",
      "People love to say things like, what if your parents blow through their money or what if they get really unhealthy and need it for medical expenses or what if the housing market collapses?\n",
      "\n",
      "They’ll say anything to detract from what is one legitimate component of many of our future financial landscapes and if you believe in such a thing, retirements.\n",
      "\n",
      "Yet, they go rhetorically crazy on you when you say you prefer to keep your money in cash rather than the stock market.\n",
      "\n",
      "It doesn’t make any sense. Not to me, at least.\n",
      "\n",
      "Counting on inheriting a house, cash, or anything else comes with risk. But it’s no different than any of the other risks we consider when we look ahead to how money — or a lack thereof — will shape our futures.\n",
      "\n",
      "I wrote about risk in a recent Medium article. It’s relevant to the matter at hand:\n",
      "\n",
      "Simply put, you must manage risk no matter the strategy you adopt. Because no matter the strategy you adopt, there will be risk. You can’t tell me my risk is worse than yours. Just as I can’t tell you my risk is less risky than yours. It’s almost wholly subjective unless your strategy is blindly day trading stocks or something. Risk isn’t some objective thing we compete over.\n",
      "\n",
      "I’m willing to forgo aggressive investing in the stock market. I opt for a cash-focused strategy alongside a low cost of living.\n",
      "\n",
      "This alone affords me the life I want to live now and intend to continue to live as I grow older. That’s enjoying each day and traveling with my amazing partner while ensuring my 17-year old daughter morphs into a north of decent adult human.\n",
      "\n",
      "I’m also counting on a low six-figure windfall when my parents pass. Because I live simply (but still like nice things!), one or two hundred grand will go incredibly far for me 10 or 20 years down the line. I tend to label it a wildcard, but one I am quite certain will be there to enhance my situation with money at what will likely be a pivotal time in my life.\n",
      "\n",
      "Not an easy subject to talk about. It’s even taboo in some circles. However, it’s important to take it into account because assessing your anticipated inheritance does and absolutely should influence the money decisions you make while your parents are still alive.\n",
      "\n",
      "If you choose to ignore this element, you’re not budgeting, saving, and spending as effectively and smartly as you otherwise could.\n",
      "\n",
      "\n",
      "This article belong to the following segments Saving, Retirement, Personal Finance, Inheritance, Money'\n",
      "'>>> Label: If You’re Under 50, You Might Not Need To Save for Retirement\n",
      "\n",
      "\n",
      "The other day my Mother gave me an update on what houses in my parents’ neighborhood have been going for. Based on this rough assessment, the house they live in would likely fetch around $150,000 today.\n",
      "\n",
      "My parents bought their house for $85,000 about 28 years ago.\n",
      "\n",
      "If you picked up their house, slapped on a Victorian facade, and moved it to the San Francisco neighborhood where I spent the last few days, it would go for well over a million dollars. In pretty much any neighborhood in The City, you’re looking at $1,000,000. In most parts of California, you could probably ask and get no less than half a million.\n",
      "\n",
      "Location, location, location!\n",
      "\n",
      "The house my parents live in, along with what I think is a modest amount of cash. That’s my inheritance.\n",
      "\n",
      "I’m probably looking at approximately $200,000 to $250,000 total.\n",
      "\n",
      "Here’s the upside — I have an insanely low cost of living that I expect to continue to go lower.\n",
      "\n",
      "I live in Los Angeles. I spend relatively very little on rent. I have a modest car. As my income continues to increase (fingers crossed!), these expenses look to trend even lower.\n",
      "\n",
      "I’m in a solid situation. I look at $100,000 or $200,000 the way other people — with higher costs of living — might look at a million dollars.\n",
      "\n",
      "This is the one thing we don’t fully comprehend about having a low cost of living.\n",
      "\n",
      "We riff condescendingly about inflation. We platitude about making our money go further. But we don’t consider how a low cost of living changes how we value money and decisions such as whether or not to be in the stock market:\n",
      "\n",
      "If you make the decision to get out of the stock market, it should have very little, if anything, to do with making or losing money. It should have everything to do with a realization that you can meet your needs — and then some — with consistent cash flow and a focus on using money to essentially love yourself and love the ones you’re with.\n",
      "\n",
      "Which leads directly to the touchy subject of expecting or counting on or considering however you choose to consider your inheritance (assuming you’re going to get one).\n",
      "\n",
      "As I detailed in a November 2020, Making of a Millionaire article, young people stand to inherit a considerable amount of cash over the next several decades.\n",
      "\n",
      "Call me a callous tool, but this absolutely figures into my financial planning:\n",
      "\n",
      "…I have a pretty good idea how much money I stand to inherit when my parents pass away. My dad just turned 86. My mom is 73. Here’s hoping the inevitable doesn’t happen for a good 20 or 30 years (knock on wood!). But it’s going to happen. Even if I don’t run around thinking about it every day, the relative certainty of receiving what will probably be somewhere in the low-six figures plays into my long-term financial planning. It has to. I’d be stupid to ignore it.\n",
      "\n",
      "The more I think about it, the more I think if you’re in a situation similar to or better than mine, you’d be really stupid to not factor an inheritance into your financial planning.\n",
      "\n",
      "In fact, if you’re under 50, it could be one of the cornerstones of your personal financial situation as you enter relative old age. It’s another reason why young people have the modern luxury of eschewing the notion of traditional retirement.\n",
      "\n",
      "In other words, you don’t have to save for retirement. You save for living.\n",
      "\n",
      "This is not to say you run reckless with your money because you know you’re getting a windfall at some still unknown date in the future. That would be really, really stupid.\n",
      "\n",
      "However, it is to say it’s perfectly okay to acknowledge that the money you’ll likely receive can, probably will, and definitely should have a material impact on your life and future, even if it simply means it lends a hand in facilitating the future you envision.\n",
      "\n",
      "You get a lot of backlash when you talk like this. When you factor in the money your parents or some other relative or friend will leave you.\n",
      "\n",
      "People love to say things like, what if your parents blow through their money or what if they get really unhealthy and need it for medical expenses or what if the housing market collapses?\n",
      "\n",
      "They’ll say anything to detract from what is one legitimate component of many of our future financial landscapes and if you believe in such a thing, retirements.\n",
      "\n",
      "Yet, they go rhetorically crazy on you when you say you prefer to keep your money in cash rather than the stock market.\n",
      "\n",
      "It doesn’t make any sense. Not to me, at least.\n",
      "\n",
      "Counting on inheriting a house, cash, or anything else comes with risk. But it’s no different than any of the other risks we consider when we look ahead to how money — or a lack thereof — will shape our futures.\n",
      "\n",
      "I wrote about risk in a recent Medium article. It’s relevant to the matter at hand:\n",
      "\n",
      "Simply put, you must manage risk no matter the strategy you adopt. Because no matter the strategy you adopt, there will be risk. You can’t tell me my risk is worse than yours. Just as I can’t tell you my risk is less risky than yours. It’s almost wholly subjective unless your strategy is blindly day trading stocks or something. Risk isn’t some objective thing we compete over.\n",
      "\n",
      "I’m willing to forgo aggressive investing in the stock market. I opt for a cash-focused strategy alongside a low cost of living.\n",
      "\n",
      "This alone affords me the life I want to live now and intend to continue to live as I grow older. That’s enjoying each day and traveling with my amazing partner while ensuring my 17-year old daughter morphs into a north of decent adult human.\n",
      "\n",
      "I’m also counting on a low six-figure windfall when my parents pass. Because I live simply (but still like nice things!), one or two hundred grand will go incredibly far for me 10 or 20 years down the line. I tend to label it a wildcard, but one I am quite certain will be there to enhance my situation with money at what will likely be a pivotal time in my life.\n",
      "\n",
      "Not an easy subject to talk about. It’s even taboo in some circles. However, it’s important to take it into account because assessing your anticipated inheritance does and absolutely should influence the money decisions you make while your parents are still alive.\n",
      "\n",
      "If you choose to ignore this element, you’re not budgeting, saving, and spending as effectively and smartly as you otherwise could.\n",
      "\n",
      "\n",
      "This article belong to the following segments Saving, Retirement, Personal Finance, Inheritance, Money'\n",
      "\n",
      "'>>> Review: AYS Daily Digest 28/06/21 — Greek Government Forces Journalists to Delete Footage, Continuing Attacks on Civil Society\n",
      "\n",
      "\n",
      "AYS Daily Digest 28/06/21 — Greek Government Forces Journalists to Delete Footage, Continuing Attacks on Civil Society\n",
      "\n",
      "Protests in Calais against the government’s ban on food distribution. Photo credit: Refugee Info Bus\n",
      "\n",
      "FEATURE\n",
      "\n",
      "Greek Government Continues Attacks on Civil Society By Forcing Journalists to Delete Footage\n",
      "\n",
      "Journalists from an as-yet-unnamed international TV channel were stopped by Greek Coast Guard security in Lesvos. The security forced them to delete hours of footage from their memory card, probably of scenes exposing the truth about border practices.\n",
      "\n",
      "This is far from an isolated incident. The authorities in Greece have been ramping up harassment and intimidation of journalists and other members of civil society, such as NGOs. There have been several incidents of authorities forcing journalists to delete footage or stopping them from freely reporting on conditions in the camps.\n",
      "\n",
      "Dutch journalist Ingeborg Beugel was even arrested for hosting an asylum seeker in her home. She is still waiting for her trial. Legal analysts say that the law under which she was arrested is rarely applied and Beugel herself says that she was most likely targeted for her journalism, which challenges fascist actors on the island of Hydra where she lives.\n",
      "\n",
      "Meanwhile, tabloids and pro-government sites freely publish misinformation, or rampant lies, about people on the move and solidarity workers. The newspaper Parapolitika repeated a report that accused supporters of the Moria 6 who protested in front of the court of arson, trafficking, and all sorts of crimes. The “report” particularly targeted journalists and legal observers that attempted to enter the courtroom, as was their right by law.\n",
      "\n",
      "Amid this tightening landscape for journalists, particularly journalists that wish to publish the truth and act in solidarity with people on the move, our BVMN colleagues disinfaux collective are publishing a new site including analysis on the events in Nea Smyrni earlier this year. Please support their work as their brave journalism is much needed.\n",
      "\n",
      "\n",
      "This article belong to the following segments Greece, Digest, Journalism, Migrants, Refugees'\n",
      "'>>> Label: AYS Daily Digest 28/06/21 — Greek Government Forces Journalists to Delete Footage, Continuing Attacks on Civil Society\n",
      "\n",
      "\n",
      "AYS Daily Digest 28/06/21 — Greek Government Forces Journalists to Delete Footage, Continuing Attacks on Civil Society\n",
      "\n",
      "Protests in Calais against the government’s ban on food distribution. Photo credit: Refugee Info Bus\n",
      "\n",
      "FEATURE\n",
      "\n",
      "Greek Government Continues Attacks on Civil Society By Forcing Journalists to Delete Footage\n",
      "\n",
      "Journalists from an as-yet-unnamed international TV channel were stopped by Greek Coast Guard security in Lesvos. The security forced them to delete hours of footage from their memory card, probably of scenes exposing the truth about border practices.\n",
      "\n",
      "This is far from an isolated incident. The authorities in Greece have been ramping up harassment and intimidation of journalists and other members of civil society, such as NGOs. There have been several incidents of authorities forcing journalists to delete footage or stopping them from freely reporting on conditions in the camps.\n",
      "\n",
      "Dutch journalist Ingeborg Beugel was even arrested for hosting an asylum seeker in her home. She is still waiting for her trial. Legal analysts say that the law under which she was arrested is rarely applied and Beugel herself says that she was most likely targeted for her journalism, which challenges fascist actors on the island of Hydra where she lives.\n",
      "\n",
      "Meanwhile, tabloids and pro-government sites freely publish misinformation, or rampant lies, about people on the move and solidarity workers. The newspaper Parapolitika repeated a report that accused supporters of the Moria 6 who protested in front of the court of arson, trafficking, and all sorts of crimes. The “report” particularly targeted journalists and legal observers that attempted to enter the courtroom, as was their right by law.\n",
      "\n",
      "Amid this tightening landscape for journalists, particularly journalists that wish to publish the truth and act in solidarity with people on the move, our BVMN colleagues disinfaux collective are publishing a new site including analysis on the events in Nea Smyrni earlier this year. Please support their work as their brave journalism is much needed.\n",
      "\n",
      "\n",
      "This article belong to the following segments Greece, Digest, Journalism, Migrants, Refugees'\n"
     ]
    }
   ],
   "source": [
    "sample = bert_dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\"\\n'>>> Review: {row['bert_text']}'\")\n",
    "    print(f\"'>>> Label: {row['bert_text']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36f33ed30394a34bda8d8eaa26da5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1800 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"bert_text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = bert_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"bert_text\", \"label\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group and split the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "512 is too huge for the GPU, we'll go with the smaller number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before Chunking the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Review 0 length: 1800'\n",
      "'>>> Review 1 length: 1851'\n",
      "'>>> Review 2 length: 875'\n"
     ]
    }
   ],
   "source": [
    "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Review {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Concatenated reviews length: 4526'\n"
     ]
    }
   ],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 80'\n"
     ]
    }
   ],
   "source": [
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed5c01024ada42c3a221225f0af0e1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 18321\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'that should be closely emphasized ; loving mentions that the word “ mercy ” is not the one that is italicized in the first line for a reason — because it is not the prevailing sentiment. loving also brings attention to the duplicity “ benighted ” carries, and how it is a euphemism for ignorant yet literally translate to “ surrounded by darkness. ” the darkness, argues loving, is not her pre - christian existence ; it is her abduction from her homeland by slave traders. the emphasis on the word “ saviour ” highlights the distinction from god himself, and loving claims that this distinction is a satirical reference to'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][900][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'that should be closely emphasized ; loving mentions that the word “ mercy ” is not the one that is italicized in the first line for a reason — because it is not the prevailing sentiment. loving also brings attention to the duplicity “ benighted ” carries, and how it is a euphemism for ignorant yet literally translate to “ surrounded by darkness. ” the darkness, argues loving, is not her pre - christian existence ; it is her abduction from her homeland by slave traders. the emphasis on the word “ saviour ” highlights the distinction from god himself, and loving claims that this distinction is a satirical reference to'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][900][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning DistilBERT with the Huggingface Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] tag helpers in asp. net core mvc — sagar jaybhay asp pageuria helper / [MASK] helpers in [MASK] [MASK]. net core mvc [MASK] [MASK] new in asp. [MASK] core. these are a server - side component. it is processed on the server and renders an html element in razor files. it is similar [MASK] html help [MASK] there are so many [MASK] helpers like generating the link, creating forms, etc, to use tag helper we need to import these tag helper first. for example, we need to create anchor [MASK] [MASK] with the help of tag helper < a'\n",
      "\n",
      "'>>> asp - controller = \" duncan \" asp - [MASK] = \" details \" asp - route - id = \" @ stud. studentid \" > details < / a > in this asp - controller = is took controller name is took controller name asp - action [MASK] is the action or [MASK] name present in that controller is the action [MASK] method name present in that controller asp [MASK] route [MASK] id = is the parameter we pass to that method [MASK] you can see if we use packed - helper the [MASK] is changed syntax is good [MASK] [MASK]. now we can see the same example, by html helper. @ [MASK].'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole Word Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] tag helpers in asp. net [MASK] mvc — sagar jaybhay [MASK] [MASK] page [MASK] helper [MASK] tag helpers [MASK] asp. net [MASK] mvc this is new in asp. net [MASK]. these are [MASK] server - side component. it is processed on [MASK] server and renders an [MASK] element in [MASK] files. it is similar to html helpers [MASK] are so many tag [MASK] [MASK] like generating the link [MASK] creating forms, etc, to use tag helper [MASK] [MASK] [MASK] import these tag helper first. for example, we need to create anchor a tag with the help of [MASK] [MASK] [MASK] < a'\n",
      "\n",
      "'>>> asp [MASK] [MASK] = \" home \" [MASK] [MASK] - [MASK] = [MASK] details \" [MASK] [MASK] - route - [MASK] = \" @ stud. [MASK] [MASK] \" > [MASK] < [MASK] a > in this [MASK] [MASK] - controller = is took controller name is took controller name asp [MASK] action = is the action or method name present in that controller is the action or [MASK] [MASK] present in that controller asp - [MASK] - id [MASK] is the parameter [MASK] [MASK] to [MASK] method. you can see if we use tag [MASK] [MASK] [MASK] the color is changed syntax is good and clean. now we can see the same example [MASK] [MASK] html helper. @ html.'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 16655\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1665\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(lm_datasets[\"train\"].shape[0]/1.1)\n",
    "test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "\n",
    "\n",
    "print(lm_datasets[\"train\"].shape[0])\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe9a68141764e9e97f47b68d44ac901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/17 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369af2ff2bf34e8391e968ca0db7a5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "downsampled_dataset.save_to_disk('MediumArticlesDataSet4MLMTraining')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 16034\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1603\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DatasetDict.load_from_disk('MediumArticlesDataSet4MLMTraining')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 32\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-MediumArticlesMLM\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=False,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    "    num_train_epochs = 50,\n",
    "    load_best_model_at_end=True,\n",
    "    save_strategy= 'epoch',\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    greater_is_better = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, EarlyStoppingCallback, IntervalStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 01:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 27.09\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 16034\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 25100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25100' max='25100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25100/25100 1:36:57, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.751600</td>\n",
       "      <td>2.542353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.612900</td>\n",
       "      <td>2.505242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.562500</td>\n",
       "      <td>2.452511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.516400</td>\n",
       "      <td>2.437785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.477500</td>\n",
       "      <td>2.405717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.445700</td>\n",
       "      <td>2.419353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.426400</td>\n",
       "      <td>2.376204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.408300</td>\n",
       "      <td>2.344040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.380200</td>\n",
       "      <td>2.379923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.354000</td>\n",
       "      <td>2.394961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.342400</td>\n",
       "      <td>2.385164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.330600</td>\n",
       "      <td>2.358645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.304000</td>\n",
       "      <td>2.350868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.290300</td>\n",
       "      <td>2.341634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.259800</td>\n",
       "      <td>2.337166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.259000</td>\n",
       "      <td>2.328170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.248400</td>\n",
       "      <td>2.334898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.234300</td>\n",
       "      <td>2.298807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.225600</td>\n",
       "      <td>2.321241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.206700</td>\n",
       "      <td>2.296066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.193600</td>\n",
       "      <td>2.301944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.177600</td>\n",
       "      <td>2.290956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.171800</td>\n",
       "      <td>2.299638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.166000</td>\n",
       "      <td>2.282393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.153200</td>\n",
       "      <td>2.296058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.142900</td>\n",
       "      <td>2.264341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.132000</td>\n",
       "      <td>2.279764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.133200</td>\n",
       "      <td>2.279757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.120100</td>\n",
       "      <td>2.267356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.116200</td>\n",
       "      <td>2.283116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.111300</td>\n",
       "      <td>2.304079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.105500</td>\n",
       "      <td>2.308323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.101500</td>\n",
       "      <td>2.249571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.090600</td>\n",
       "      <td>2.295495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.086400</td>\n",
       "      <td>2.278257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.081300</td>\n",
       "      <td>2.271440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.091200</td>\n",
       "      <td>2.234169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.067900</td>\n",
       "      <td>2.277145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.064300</td>\n",
       "      <td>2.279839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.068800</td>\n",
       "      <td>2.234153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.055100</td>\n",
       "      <td>2.247904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.061400</td>\n",
       "      <td>2.230593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.051700</td>\n",
       "      <td>2.262967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.048400</td>\n",
       "      <td>2.258975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.053200</td>\n",
       "      <td>2.277974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.043300</td>\n",
       "      <td>2.248677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.053800</td>\n",
       "      <td>2.252967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.044400</td>\n",
       "      <td>2.218862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2.045200</td>\n",
       "      <td>2.230535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.038800</td>\n",
       "      <td>2.222864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-502\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-502\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-502\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-1004\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-1004\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-1004\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-1506\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-1506\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-1506\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-2008\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-2008\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-2008\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-2510\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-2510\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-2510\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-3012\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-3012\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-3012\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-3514\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-3514\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-3514\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-4016\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-4016\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-4016\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-4518\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-4518\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-4518\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-5020\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-5020\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-5020\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-5522\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-5522\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-5522\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-6024\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-6024\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-6024\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-6526\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-6526\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-6526\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-7028\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-7028\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-7028\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-7530\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-7530\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-7530\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-8032\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-8032\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-8032\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-8534\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-8534\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-8534\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-9036\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-9036\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-9036\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-9538\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-9538\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-9538\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-10040\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-10040\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-10040\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-10542\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-10542\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-10542\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-11044\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-11044\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-11044\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-11546\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-11546\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-11546\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-12048\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-12048\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-12048\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-12550\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-12550\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-12550\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-13052\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-13052\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-13052\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-13554\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-13554\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-13554\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-14056\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-14056\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-14056\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-14558\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-14558\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-14558\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-15060\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-15060\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-15060\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-15562\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-15562\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-15562\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-16064\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-16064\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-16064\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-16566\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-16566\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-16566\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-17068\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-17068\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-17068\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-17570\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-17570\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-17570\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-18072\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-18072\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-18072\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-18574\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-18574\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-18574\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-19076\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-19076\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-19076\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-19578\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-19578\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-19578\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-20080\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-20080\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-20080\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-20582\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-20582\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-20582\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-21084\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-21084\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-21084\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-21586\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-21586\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-21586\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-22088\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-22088\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-22088\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-22590\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-22590\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-22590\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-23092\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-23092\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-23092\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-23594\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-23594\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-23594\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-24096\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-24096\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-24096\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-24598\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-24598\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-24598\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-uncased-MediumArticlesMLM\\checkpoint-25100\n",
      "Configuration saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-25100\\config.json\n",
      "Model weights saved in distilbert-base-uncased-MediumArticlesMLM\\checkpoint-25100\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from distilbert-base-uncased-MediumArticlesMLM\\checkpoint-24096 (score: 2.2188620567321777).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25100, training_loss=2.2097839118379996, metrics={'train_runtime': 5818.4349, 'train_samples_per_second': 137.786, 'train_steps_per_second': 4.314, 'total_flos': 2.65685697105408e+16, 'train_loss': 2.2097839118379996, 'epoch': 50.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2188620567321777"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'distilbert-base-uncased-MediumArticlesMLM\\\\checkpoint-24096'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[{'loss': 2.7516,\n",
       "  'learning_rate': 1.9602390438247013e-05,\n",
       "  'epoch': 1.0,\n",
       "  'step': 501},\n",
       " {'eval_loss': 2.5423526763916016,\n",
       "  'eval_runtime': 2.2961,\n",
       "  'eval_samples_per_second': 698.147,\n",
       "  'eval_steps_per_second': 22.212,\n",
       "  'epoch': 1.0,\n",
       "  'step': 502},\n",
       " {'loss': 2.6129,\n",
       "  'learning_rate': 1.920318725099602e-05,\n",
       "  'epoch': 2.0,\n",
       "  'step': 1002},\n",
       " {'eval_loss': 2.505242347717285,\n",
       "  'eval_runtime': 2.7092,\n",
       "  'eval_samples_per_second': 591.678,\n",
       "  'eval_steps_per_second': 18.824,\n",
       "  'epoch': 2.0,\n",
       "  'step': 1004},\n",
       " {'loss': 2.5625,\n",
       "  'learning_rate': 1.880398406374502e-05,\n",
       "  'epoch': 2.99,\n",
       "  'step': 1503},\n",
       " {'eval_loss': 2.4525105953216553,\n",
       "  'eval_runtime': 2.9555,\n",
       "  'eval_samples_per_second': 542.387,\n",
       "  'eval_steps_per_second': 17.256,\n",
       "  'epoch': 3.0,\n",
       "  'step': 1506},\n",
       " {'loss': 2.5164,\n",
       "  'learning_rate': 1.8404780876494026e-05,\n",
       "  'epoch': 3.99,\n",
       "  'step': 2004},\n",
       " {'eval_loss': 2.4377851486206055,\n",
       "  'eval_runtime': 2.9322,\n",
       "  'eval_samples_per_second': 546.693,\n",
       "  'eval_steps_per_second': 17.393,\n",
       "  'epoch': 4.0,\n",
       "  'step': 2008},\n",
       " {'loss': 2.4775,\n",
       "  'learning_rate': 1.800557768924303e-05,\n",
       "  'epoch': 4.99,\n",
       "  'step': 2505},\n",
       " {'eval_loss': 2.40571665763855,\n",
       "  'eval_runtime': 2.9429,\n",
       "  'eval_samples_per_second': 544.708,\n",
       "  'eval_steps_per_second': 17.33,\n",
       "  'epoch': 5.0,\n",
       "  'step': 2510},\n",
       " {'loss': 2.4457,\n",
       "  'learning_rate': 1.7606374501992033e-05,\n",
       "  'epoch': 5.99,\n",
       "  'step': 3006},\n",
       " {'eval_loss': 2.4193527698516846,\n",
       "  'eval_runtime': 3.9378,\n",
       "  'eval_samples_per_second': 407.077,\n",
       "  'eval_steps_per_second': 12.951,\n",
       "  'epoch': 6.0,\n",
       "  'step': 3012},\n",
       " {'loss': 2.4264,\n",
       "  'learning_rate': 1.720717131474104e-05,\n",
       "  'epoch': 6.99,\n",
       "  'step': 3507},\n",
       " {'eval_loss': 2.376203775405884,\n",
       "  'eval_runtime': 3.4481,\n",
       "  'eval_samples_per_second': 464.891,\n",
       "  'eval_steps_per_second': 14.791,\n",
       "  'epoch': 7.0,\n",
       "  'step': 3514},\n",
       " {'loss': 2.4083,\n",
       "  'learning_rate': 1.680796812749004e-05,\n",
       "  'epoch': 7.98,\n",
       "  'step': 4008},\n",
       " {'eval_loss': 2.3440403938293457,\n",
       "  'eval_runtime': 3.3927,\n",
       "  'eval_samples_per_second': 472.49,\n",
       "  'eval_steps_per_second': 15.032,\n",
       "  'epoch': 8.0,\n",
       "  'step': 4016},\n",
       " {'loss': 2.3802,\n",
       "  'learning_rate': 1.640956175298805e-05,\n",
       "  'epoch': 8.98,\n",
       "  'step': 4509},\n",
       " {'eval_loss': 2.3799233436584473,\n",
       "  'eval_runtime': 3.1555,\n",
       "  'eval_samples_per_second': 508.005,\n",
       "  'eval_steps_per_second': 16.162,\n",
       "  'epoch': 9.0,\n",
       "  'step': 4518},\n",
       " {'loss': 2.354,\n",
       "  'learning_rate': 1.6011155378486058e-05,\n",
       "  'epoch': 9.98,\n",
       "  'step': 5010},\n",
       " {'eval_loss': 2.394961357116699,\n",
       "  'eval_runtime': 4.2698,\n",
       "  'eval_samples_per_second': 375.43,\n",
       "  'eval_steps_per_second': 11.944,\n",
       "  'epoch': 10.0,\n",
       "  'step': 5020},\n",
       " {'loss': 2.3424,\n",
       "  'learning_rate': 1.561195219123506e-05,\n",
       "  'epoch': 10.98,\n",
       "  'step': 5511},\n",
       " {'eval_loss': 2.3851635456085205,\n",
       "  'eval_runtime': 4.1853,\n",
       "  'eval_samples_per_second': 383.009,\n",
       "  'eval_steps_per_second': 12.186,\n",
       "  'epoch': 11.0,\n",
       "  'step': 5522},\n",
       " {'loss': 2.3306,\n",
       "  'learning_rate': 1.5212749003984063e-05,\n",
       "  'epoch': 11.98,\n",
       "  'step': 6012},\n",
       " {'eval_loss': 2.358644962310791,\n",
       "  'eval_runtime': 3.8747,\n",
       "  'eval_samples_per_second': 413.712,\n",
       "  'eval_steps_per_second': 13.162,\n",
       "  'epoch': 12.0,\n",
       "  'step': 6024},\n",
       " {'loss': 2.304,\n",
       "  'learning_rate': 1.4813545816733069e-05,\n",
       "  'epoch': 12.97,\n",
       "  'step': 6513},\n",
       " {'eval_loss': 2.350867509841919,\n",
       "  'eval_runtime': 4.0798,\n",
       "  'eval_samples_per_second': 392.907,\n",
       "  'eval_steps_per_second': 12.5,\n",
       "  'epoch': 13.0,\n",
       "  'step': 6526},\n",
       " {'loss': 2.2903,\n",
       "  'learning_rate': 1.4414342629482072e-05,\n",
       "  'epoch': 13.97,\n",
       "  'step': 7014},\n",
       " {'eval_loss': 2.3416342735290527,\n",
       "  'eval_runtime': 3.5348,\n",
       "  'eval_samples_per_second': 453.49,\n",
       "  'eval_steps_per_second': 14.428,\n",
       "  'epoch': 14.0,\n",
       "  'step': 7028},\n",
       " {'loss': 2.2598,\n",
       "  'learning_rate': 1.4015139442231076e-05,\n",
       "  'epoch': 14.97,\n",
       "  'step': 7515},\n",
       " {'eval_loss': 2.337165594100952,\n",
       "  'eval_runtime': 3.6058,\n",
       "  'eval_samples_per_second': 444.559,\n",
       "  'eval_steps_per_second': 14.144,\n",
       "  'epoch': 15.0,\n",
       "  'step': 7530},\n",
       " {'loss': 2.259,\n",
       "  'learning_rate': 1.361593625498008e-05,\n",
       "  'epoch': 15.97,\n",
       "  'step': 8016},\n",
       " {'eval_loss': 2.328169822692871,\n",
       "  'eval_runtime': 3.5949,\n",
       "  'eval_samples_per_second': 445.911,\n",
       "  'eval_steps_per_second': 14.187,\n",
       "  'epoch': 16.0,\n",
       "  'step': 8032},\n",
       " {'loss': 2.2484,\n",
       "  'learning_rate': 1.3216733067729083e-05,\n",
       "  'epoch': 16.97,\n",
       "  'step': 8517},\n",
       " {'eval_loss': 2.3348984718322754,\n",
       "  'eval_runtime': 3.609,\n",
       "  'eval_samples_per_second': 444.163,\n",
       "  'eval_steps_per_second': 14.131,\n",
       "  'epoch': 17.0,\n",
       "  'step': 8534},\n",
       " {'loss': 2.2343,\n",
       "  'learning_rate': 1.2818326693227092e-05,\n",
       "  'epoch': 17.96,\n",
       "  'step': 9018},\n",
       " {'eval_loss': 2.298807144165039,\n",
       "  'eval_runtime': 3.6875,\n",
       "  'eval_samples_per_second': 434.71,\n",
       "  'eval_steps_per_second': 13.83,\n",
       "  'epoch': 18.0,\n",
       "  'step': 9036},\n",
       " {'loss': 2.2256,\n",
       "  'learning_rate': 1.2419123505976096e-05,\n",
       "  'epoch': 18.96,\n",
       "  'step': 9519},\n",
       " {'eval_loss': 2.3212406635284424,\n",
       "  'eval_runtime': 3.8136,\n",
       "  'eval_samples_per_second': 420.337,\n",
       "  'eval_steps_per_second': 13.373,\n",
       "  'epoch': 19.0,\n",
       "  'step': 9538},\n",
       " {'loss': 2.2067,\n",
       "  'learning_rate': 1.20199203187251e-05,\n",
       "  'epoch': 19.96,\n",
       "  'step': 10020},\n",
       " {'eval_loss': 2.2960662841796875,\n",
       "  'eval_runtime': 3.6572,\n",
       "  'eval_samples_per_second': 438.309,\n",
       "  'eval_steps_per_second': 13.945,\n",
       "  'epoch': 20.0,\n",
       "  'step': 10040},\n",
       " {'loss': 2.1936,\n",
       "  'learning_rate': 1.1621513944223108e-05,\n",
       "  'epoch': 20.96,\n",
       "  'step': 10521},\n",
       " {'eval_loss': 2.3019440174102783,\n",
       "  'eval_runtime': 3.6839,\n",
       "  'eval_samples_per_second': 435.133,\n",
       "  'eval_steps_per_second': 13.844,\n",
       "  'epoch': 21.0,\n",
       "  'step': 10542},\n",
       " {'loss': 2.1776,\n",
       "  'learning_rate': 1.1222310756972113e-05,\n",
       "  'epoch': 21.96,\n",
       "  'step': 11022},\n",
       " {'eval_loss': 2.2909562587738037,\n",
       "  'eval_runtime': 3.591,\n",
       "  'eval_samples_per_second': 446.39,\n",
       "  'eval_steps_per_second': 14.202,\n",
       "  'epoch': 22.0,\n",
       "  'step': 11044},\n",
       " {'loss': 2.1718,\n",
       "  'learning_rate': 1.0823107569721117e-05,\n",
       "  'epoch': 22.95,\n",
       "  'step': 11523},\n",
       " {'eval_loss': 2.299638032913208,\n",
       "  'eval_runtime': 3.67,\n",
       "  'eval_samples_per_second': 436.786,\n",
       "  'eval_steps_per_second': 13.897,\n",
       "  'epoch': 23.0,\n",
       "  'step': 11546},\n",
       " {'loss': 2.166,\n",
       "  'learning_rate': 1.042390438247012e-05,\n",
       "  'epoch': 23.95,\n",
       "  'step': 12024},\n",
       " {'eval_loss': 2.282392978668213,\n",
       "  'eval_runtime': 3.5909,\n",
       "  'eval_samples_per_second': 446.4,\n",
       "  'eval_steps_per_second': 14.202,\n",
       "  'epoch': 24.0,\n",
       "  'step': 12048},\n",
       " {'loss': 2.1532,\n",
       "  'learning_rate': 1.0024701195219124e-05,\n",
       "  'epoch': 24.95,\n",
       "  'step': 12525},\n",
       " {'eval_loss': 2.296057939529419,\n",
       "  'eval_runtime': 3.558,\n",
       "  'eval_samples_per_second': 450.533,\n",
       "  'eval_steps_per_second': 14.334,\n",
       "  'epoch': 25.0,\n",
       "  'step': 12550},\n",
       " {'loss': 2.1429,\n",
       "  'learning_rate': 9.625498007968128e-06,\n",
       "  'epoch': 25.95,\n",
       "  'step': 13026},\n",
       " {'eval_loss': 2.264341354370117,\n",
       "  'eval_runtime': 3.4526,\n",
       "  'eval_samples_per_second': 464.282,\n",
       "  'eval_steps_per_second': 14.771,\n",
       "  'epoch': 26.0,\n",
       "  'step': 13052},\n",
       " {'loss': 2.132,\n",
       "  'learning_rate': 9.226294820717131e-06,\n",
       "  'epoch': 26.95,\n",
       "  'step': 13527},\n",
       " {'eval_loss': 2.279764175415039,\n",
       "  'eval_runtime': 3.6095,\n",
       "  'eval_samples_per_second': 444.107,\n",
       "  'eval_steps_per_second': 14.129,\n",
       "  'epoch': 27.0,\n",
       "  'step': 13554},\n",
       " {'loss': 2.1332,\n",
       "  'learning_rate': 8.827091633466137e-06,\n",
       "  'epoch': 27.94,\n",
       "  'step': 14028},\n",
       " {'eval_loss': 2.279757022857666,\n",
       "  'eval_runtime': 3.6029,\n",
       "  'eval_samples_per_second': 444.922,\n",
       "  'eval_steps_per_second': 14.155,\n",
       "  'epoch': 28.0,\n",
       "  'step': 14056},\n",
       " {'loss': 2.1201,\n",
       "  'learning_rate': 8.428685258964144e-06,\n",
       "  'epoch': 28.94,\n",
       "  'step': 14529},\n",
       " {'eval_loss': 2.2673556804656982,\n",
       "  'eval_runtime': 3.5776,\n",
       "  'eval_samples_per_second': 448.063,\n",
       "  'eval_steps_per_second': 14.255,\n",
       "  'epoch': 29.0,\n",
       "  'step': 14558},\n",
       " {'loss': 2.1162,\n",
       "  'learning_rate': 8.030278884462152e-06,\n",
       "  'epoch': 29.94,\n",
       "  'step': 15030},\n",
       " {'eval_loss': 2.2831156253814697,\n",
       "  'eval_runtime': 3.6556,\n",
       "  'eval_samples_per_second': 438.506,\n",
       "  'eval_steps_per_second': 13.951,\n",
       "  'epoch': 30.0,\n",
       "  'step': 15060},\n",
       " {'loss': 2.1113,\n",
       "  'learning_rate': 7.631075697211156e-06,\n",
       "  'epoch': 30.94,\n",
       "  'step': 15531},\n",
       " {'eval_loss': 2.304079294204712,\n",
       "  'eval_runtime': 3.4968,\n",
       "  'eval_samples_per_second': 458.417,\n",
       "  'eval_steps_per_second': 14.585,\n",
       "  'epoch': 31.0,\n",
       "  'step': 15562},\n",
       " {'loss': 2.1055,\n",
       "  'learning_rate': 7.2318725099601596e-06,\n",
       "  'epoch': 31.94,\n",
       "  'step': 16032},\n",
       " {'eval_loss': 2.3083229064941406,\n",
       "  'eval_runtime': 3.549,\n",
       "  'eval_samples_per_second': 451.679,\n",
       "  'eval_steps_per_second': 14.37,\n",
       "  'epoch': 32.0,\n",
       "  'step': 16064},\n",
       " {'loss': 2.1015,\n",
       "  'learning_rate': 6.832669322709163e-06,\n",
       "  'epoch': 32.93,\n",
       "  'step': 16533},\n",
       " {'eval_loss': 2.2495710849761963,\n",
       "  'eval_runtime': 3.6074,\n",
       "  'eval_samples_per_second': 444.367,\n",
       "  'eval_steps_per_second': 14.138,\n",
       "  'epoch': 33.0,\n",
       "  'step': 16566},\n",
       " {'loss': 2.0906,\n",
       "  'learning_rate': 6.433466135458168e-06,\n",
       "  'epoch': 33.93,\n",
       "  'step': 17034},\n",
       " {'eval_loss': 2.295494556427002,\n",
       "  'eval_runtime': 3.5187,\n",
       "  'eval_samples_per_second': 455.567,\n",
       "  'eval_steps_per_second': 14.494,\n",
       "  'epoch': 34.0,\n",
       "  'step': 17068},\n",
       " {'loss': 2.0864,\n",
       "  'learning_rate': 6.034262948207171e-06,\n",
       "  'epoch': 34.93,\n",
       "  'step': 17535},\n",
       " {'eval_loss': 2.27825665473938,\n",
       "  'eval_runtime': 3.6055,\n",
       "  'eval_samples_per_second': 444.598,\n",
       "  'eval_steps_per_second': 14.145,\n",
       "  'epoch': 35.0,\n",
       "  'step': 17570},\n",
       " {'loss': 2.0813,\n",
       "  'learning_rate': 5.635059760956175e-06,\n",
       "  'epoch': 35.93,\n",
       "  'step': 18036},\n",
       " {'eval_loss': 2.271440029144287,\n",
       "  'eval_runtime': 3.4616,\n",
       "  'eval_samples_per_second': 463.087,\n",
       "  'eval_steps_per_second': 14.733,\n",
       "  'epoch': 36.0,\n",
       "  'step': 18072},\n",
       " {'loss': 2.0912,\n",
       "  'learning_rate': 5.2358565737051795e-06,\n",
       "  'epoch': 36.93,\n",
       "  'step': 18537},\n",
       " {'eval_loss': 2.234168767929077,\n",
       "  'eval_runtime': 3.7126,\n",
       "  'eval_samples_per_second': 431.776,\n",
       "  'eval_steps_per_second': 13.737,\n",
       "  'epoch': 37.0,\n",
       "  'step': 18574},\n",
       " {'loss': 2.0679,\n",
       "  'learning_rate': 4.837450199203187e-06,\n",
       "  'epoch': 37.92,\n",
       "  'step': 19038},\n",
       " {'eval_loss': 2.2771449089050293,\n",
       "  'eval_runtime': 3.5029,\n",
       "  'eval_samples_per_second': 457.624,\n",
       "  'eval_steps_per_second': 14.559,\n",
       "  'epoch': 38.0,\n",
       "  'step': 19076},\n",
       " {'loss': 2.0643,\n",
       "  'learning_rate': 4.438247011952192e-06,\n",
       "  'epoch': 38.92,\n",
       "  'step': 19539},\n",
       " {'eval_loss': 2.279839277267456,\n",
       "  'eval_runtime': 3.5987,\n",
       "  'eval_samples_per_second': 445.443,\n",
       "  'eval_steps_per_second': 14.172,\n",
       "  'epoch': 39.0,\n",
       "  'step': 19578},\n",
       " {'loss': 2.0688,\n",
       "  'learning_rate': 4.039043824701195e-06,\n",
       "  'epoch': 39.92,\n",
       "  'step': 20040},\n",
       " {'eval_loss': 2.2341525554656982,\n",
       "  'eval_runtime': 3.7842,\n",
       "  'eval_samples_per_second': 423.607,\n",
       "  'eval_steps_per_second': 13.477,\n",
       "  'epoch': 40.0,\n",
       "  'step': 20080},\n",
       " {'loss': 2.0551,\n",
       "  'learning_rate': 3.6398406374501995e-06,\n",
       "  'epoch': 40.92,\n",
       "  'step': 20541},\n",
       " {'eval_loss': 2.247904062271118,\n",
       "  'eval_runtime': 3.5282,\n",
       "  'eval_samples_per_second': 454.334,\n",
       "  'eval_steps_per_second': 14.455,\n",
       "  'epoch': 41.0,\n",
       "  'step': 20582},\n",
       " {'loss': 2.0614,\n",
       "  'learning_rate': 3.2414342629482076e-06,\n",
       "  'epoch': 41.92,\n",
       "  'step': 21042},\n",
       " {'eval_loss': 2.23059344291687,\n",
       "  'eval_runtime': 3.5834,\n",
       "  'eval_samples_per_second': 447.338,\n",
       "  'eval_steps_per_second': 14.232,\n",
       "  'epoch': 42.0,\n",
       "  'step': 21084},\n",
       " {'loss': 2.0517,\n",
       "  'learning_rate': 2.8422310756972117e-06,\n",
       "  'epoch': 42.91,\n",
       "  'step': 21543},\n",
       " {'eval_loss': 2.262967109680176,\n",
       "  'eval_runtime': 3.5024,\n",
       "  'eval_samples_per_second': 457.689,\n",
       "  'eval_steps_per_second': 14.562,\n",
       "  'epoch': 43.0,\n",
       "  'step': 21586},\n",
       " {'loss': 2.0484,\n",
       "  'learning_rate': 2.4430278884462154e-06,\n",
       "  'epoch': 43.91,\n",
       "  'step': 22044},\n",
       " {'eval_loss': 2.2589752674102783,\n",
       "  'eval_runtime': 3.5607,\n",
       "  'eval_samples_per_second': 450.194,\n",
       "  'eval_steps_per_second': 14.323,\n",
       "  'epoch': 44.0,\n",
       "  'step': 22088},\n",
       " {'loss': 2.0532,\n",
       "  'learning_rate': 2.043824701195219e-06,\n",
       "  'epoch': 44.91,\n",
       "  'step': 22545},\n",
       " {'eval_loss': 2.2779743671417236,\n",
       "  'eval_runtime': 3.4766,\n",
       "  'eval_samples_per_second': 461.08,\n",
       "  'eval_steps_per_second': 14.669,\n",
       "  'epoch': 45.0,\n",
       "  'step': 22590},\n",
       " {'loss': 2.0433,\n",
       "  'learning_rate': 1.6454183266932272e-06,\n",
       "  'epoch': 45.91,\n",
       "  'step': 23046},\n",
       " {'eval_loss': 2.2486767768859863,\n",
       "  'eval_runtime': 3.5221,\n",
       "  'eval_samples_per_second': 455.124,\n",
       "  'eval_steps_per_second': 14.48,\n",
       "  'epoch': 46.0,\n",
       "  'step': 23092},\n",
       " {'loss': 2.0538,\n",
       "  'learning_rate': 1.2462151394422312e-06,\n",
       "  'epoch': 46.91,\n",
       "  'step': 23547},\n",
       " {'eval_loss': 2.252967357635498,\n",
       "  'eval_runtime': 3.5883,\n",
       "  'eval_samples_per_second': 446.731,\n",
       "  'eval_steps_per_second': 14.213,\n",
       "  'epoch': 47.0,\n",
       "  'step': 23594},\n",
       " {'loss': 2.0444,\n",
       "  'learning_rate': 8.470119521912351e-07,\n",
       "  'epoch': 47.9,\n",
       "  'step': 24048},\n",
       " {'eval_loss': 2.2188620567321777,\n",
       "  'eval_runtime': 3.3815,\n",
       "  'eval_samples_per_second': 474.047,\n",
       "  'eval_steps_per_second': 15.082,\n",
       "  'epoch': 48.0,\n",
       "  'step': 24096},\n",
       " {'loss': 2.0452,\n",
       "  'learning_rate': 4.478087649402391e-07,\n",
       "  'epoch': 48.9,\n",
       "  'step': 24549},\n",
       " {'eval_loss': 2.230534791946411,\n",
       "  'eval_runtime': 3.6638,\n",
       "  'eval_samples_per_second': 437.518,\n",
       "  'eval_steps_per_second': 13.92,\n",
       "  'epoch': 49.0,\n",
       "  'step': 24598},\n",
       " {'loss': 2.0388,\n",
       "  'learning_rate': 4.940239043824702e-08,\n",
       "  'epoch': 49.9,\n",
       "  'step': 25050},\n",
       " {'eval_loss': 2.2228636741638184,\n",
       "  'eval_runtime': 4.0937,\n",
       "  'eval_samples_per_second': 391.581,\n",
       "  'eval_steps_per_second': 12.458,\n",
       "  'epoch': 50.0,\n",
       "  'step': 25100},\n",
       " {'train_runtime': 5818.4349,\n",
       "  'train_samples_per_second': 137.786,\n",
       "  'train_steps_per_second': 4.314,\n",
       "  'total_flos': 2.65685697105408e+16,\n",
       "  'train_loss': 2.2097839118379996,\n",
       "  'epoch': 50.0,\n",
       "  'step': 25100}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.state.best_metric\n",
    "trainer.state.best_model_checkpoint\n",
    "trainer.state.log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.DataFrame(trainer.state.log_history).groupby(['epoch'])['loss', 'eval_loss'].sum()\n",
    "\n",
    "history.reset_index(drop = False, inplace = True)\n",
    "\n",
    "history['epoch_int'] = [math.ceil(x) for x in history['epoch']]\n",
    "\n",
    "history = history.groupby('epoch_int')['loss', 'eval_loss'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ad3e7fbca0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'MLM Finetuning')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epochs==>')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss==>')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history.plot(figsize = (15,7))\n",
    "plt.title('MLM Finetuning')\n",
    "plt.xlabel('Epochs==>')\n",
    "plt.ylabel('Loss==>')\n",
    "plt.savefig('DBert_TrainValLoss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1603\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 9.33\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./MediumArticlesDbertFineTuned\\config.json\n",
      "Model weights saved in ./MediumArticlesDbertFineTuned\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('./MediumArticlesDbertFineTuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MediumArticlesDbertTokenizer\\\\tokenizer_config.json',\n",
       " 'MediumArticlesDbertTokenizer\\\\special_tokens_map.json',\n",
       " 'MediumArticlesDbertTokenizer\\\\vocab.txt',\n",
       " 'MediumArticlesDbertTokenizer\\\\added_tokens.json',\n",
       " 'MediumArticlesDbertTokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('MediumArticlesDbertTokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"MediumArticlesDbertFineTuned\"\n",
    "tokenizer_chkpt = 'MediumArticlesDbertTokenizer'\n",
    "config = DistilBertConfig(output_hidden_states = True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint, config = config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_chkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  2031,  2048,  8870,  2029,  2024,  5186, 20355,   999,\n",
      "           102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# txt = 'hello world'\n",
    "txt = 'I have two cats which are extremely naughty!'\n",
    "ip = tokenizer(txt, return_tensors= 'pt')\n",
    "print(ip)\n",
    "op = model(ip['input_ids'], ip['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '[CLS]'),\n",
       " (1045, 'i'),\n",
       " (2031, 'have'),\n",
       " (2048, 'two'),\n",
       " (8870, 'cats'),\n",
       " (2029, 'which'),\n",
       " (2024, 'are'),\n",
       " (5186, 'extremely'),\n",
       " (20355, 'naughty!'),\n",
       " (999, '[SEP]')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = 'I have two cats which are extremely naughty!'\n",
    "# txt = 'hello world'\n",
    "enc = tokenizer.encode(txt)\n",
    "dec = tokenizer.decode(enc)\n",
    "\n",
    "[(enc_, dec_) for (enc_,dec_) in zip(enc,dec.split())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "txt_lst = ['hello world', \n",
    "           'I have two cats which are extremely naughty!']\n",
    "ip = tokenizer(txt_lst, max_length= 32, padding= 'max_length', return_tensors= 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7592,  2088,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [  101,  1045,  2031,  2048,  8870,  2029,  2024,  5186, 20355,   999,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = model(ip['input_ids'], ip['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  2031,  2048,  8870,  2029,  2024,  5186, 20355,   999,\n",
      "           102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# txt = 'hello world'\n",
    "txt = 'I have two cats which are extremely naughty!'\n",
    "ip = tokenizer(txt, return_tensors= 'pt')\n",
    "print(ip)\n",
    "op = model(ip['input_ids'], ip['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(op.hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 768])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3515, -0.1593, -0.2332,  ...,  0.1622,  0.1203,  0.0880],\n",
       "         [ 0.2737,  0.3202, -0.5646,  ...,  0.4563,  0.9332,  0.3527],\n",
       "         [-0.2009,  0.3250,  0.4047,  ...,  0.6116, -1.2480, -0.5579],\n",
       "         ...,\n",
       "         [ 0.0644, -0.0431,  0.4693,  ..., -0.3132, -0.2309, -0.7068],\n",
       "         [ 1.1086, -0.1345, -0.8389,  ...,  0.3473,  0.2417,  0.4418],\n",
       "         [-0.3996, -0.0612, -0.2939,  ...,  0.0884,  0.5354,  0.0552]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op.hidden_states[0].shape\n",
    "op.hidden_states[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentenceEmbeddings(text: str = None, model = None , tokenizer=None, num_hidden_states: int = None):\n",
    "    '''\n",
    "    calculate the mean sentence embeddings of text from bert model.\n",
    "    Args:\n",
    "        param: text: string : the text that needs to be featurized\n",
    "        param: model: transformers.models: Bert model that takes Pytorch input\n",
    "        param: tokenizer: tokenizer that takes text as input and outputs input_ids and attention_mask torch tensors\n",
    "        param: num_hidden_states: int: number of hidden states to consider while for horizontal stack\n",
    "    \n",
    "    return type: Torch Tensor\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if num_hidden_states is None:\n",
    "        print(\"num_hidden_states is none!!! hence using last layer\")\n",
    "        num_hidden_states = 1\n",
    "        \n",
    "    if text is None or \"\":\n",
    "        raise ValueError(\"text cannot be None or empty string\")\n",
    "    \n",
    "    if model is None or tokenizer is None:\n",
    "        raise ValueError(\"Please Specify model and tokenizer properly\")\n",
    "    \n",
    "    if num_hidden_states > model.config.n_layers:\n",
    "        print('Number of hidden layers in model are {0}, but specified layers to consider are {1}'.format(model.config.n_layers, num_hidden_states))\n",
    "        print('Using all the layers to get Sentence Embeddings')\n",
    "    \n",
    "    # tokenize the text\n",
    "    ip = tokenizer(text, return_tensors = 'pt', truncation = True)\n",
    "    # get the model output\n",
    "    op = model(**ip)\n",
    "    #get hidden stated\n",
    "    hs = op['hidden_states'][-1*num_hidden_states:]\n",
    "    # stack the vectors horizontally\n",
    "    stack_vec= torch.hstack([vec[0] for vec in hs])\n",
    "    #get the mean embeddings\n",
    "    mean_vec = torch.mean(stack_vec, dim = 0)\n",
    "    \n",
    "    return mean_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = SentenceEmbeddings('RVCE is one of the top college in karnataka', model, tokenizer, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1536])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.reshape((1,-1)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task= 'fill-mask', model=model, tokenizer=tokenizer)\n",
    "# fill mask is used for auto correct / auto suggestions etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.7424009442329407,\n",
       "  'token': 2653,\n",
       "  'token_str': 'language',\n",
       "  'sequence': 'the fullform of nlp is natural language processing'},\n",
       " {'score': 0.0478600338101387,\n",
       "  'token': 3224,\n",
       "  'token_str': 'forest',\n",
       "  'sequence': 'the fullform of nlp is natural forest processing'},\n",
       " {'score': 0.02924789860844612,\n",
       "  'token': 4742,\n",
       "  'token_str': 'signal',\n",
       "  'sequence': 'the fullform of nlp is natural signal processing'},\n",
       " {'score': 0.021199537441134453,\n",
       "  'token': 3746,\n",
       "  'token_str': 'image',\n",
       "  'sequence': 'the fullform of nlp is natural image processing'},\n",
       " {'score': 0.008734915405511856,\n",
       "  'token': 1011,\n",
       "  'token_str': '-',\n",
       "  'sequence': 'the fullform of nlp is natural - processing'}]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe('the fullform of NLP is natural [MASK] processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.3084496259689331,\n",
       "  'token': 6364,\n",
       "  'token_str': 'processing',\n",
       "  'sequence': 'the fullform of nlp is natural language processing'},\n",
       " {'score': 0.2286016196012497,\n",
       "  'token': 1012,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'the fullform of nlp is natural language.'},\n",
       " {'score': 0.03698771819472313,\n",
       "  'token': 4083,\n",
       "  'token_str': 'learning',\n",
       "  'sequence': 'the fullform of nlp is natural language learning'},\n",
       " {'score': 0.03688197582960129,\n",
       "  'token': 2069,\n",
       "  'token_str': 'only',\n",
       "  'sequence': 'the fullform of nlp is natural language only'},\n",
       " {'score': 0.021707097068428993,\n",
       "  'token': 2671,\n",
       "  'token_str': 'science',\n",
       "  'sequence': 'the fullform of nlp is natural language science'}]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe('the fullform of NLP is natural language [MASK]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
