{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration fabiochiu--medium-articles-96791ff68926910d\n",
      "Reusing dataset csv (C:\\Users\\Sridhar Kamoji\\.cache\\huggingface\\datasets\\fabiochiu___csv\\fabiochiu--medium-articles-96791ff68926910d\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06aa8b533ad34114afddcaee29b9abff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset Ref:\n",
    "# https://huggingface.co/datasets/fabiochiu/medium-articles\n",
    "data = datasets.load_dataset('fabiochiu/medium-articles', data_files= 'medium_articles.csv')\n",
    "\n",
    "# converting the dataset_dictionary object to pandas dataframe\n",
    "data = pd.DataFrame.from_dict(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192368, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>authors</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mental Note Vol. 24</td>\n",
       "      <td>Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...</td>\n",
       "      <td>https://medium.com/invisible-illness/mental-no...</td>\n",
       "      <td>['Ryan Fan']</td>\n",
       "      <td>2020-12-26 03:38:10.479000+00:00</td>\n",
       "      <td>['Mental Health', 'Health', 'Psychology', 'Sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Your Brain On Coronavirus</td>\n",
       "      <td>Your Brain On Coronavirus\\n\\nA guide to the cu...</td>\n",
       "      <td>https://medium.com/age-of-awareness/how-the-pa...</td>\n",
       "      <td>['Simon Spichak']</td>\n",
       "      <td>2020-09-23 22:10:17.126000+00:00</td>\n",
       "      <td>['Mental Health', 'Coronavirus', 'Science', 'P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mind Your Nose</td>\n",
       "      <td>Mind Your Nose\\n\\nHow smell training can chang...</td>\n",
       "      <td>https://medium.com/neodotlife/mind-your-nose-f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020-10-10 20:17:37.132000+00:00</td>\n",
       "      <td>['Biotechnology', 'Neuroscience', 'Brain', 'We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The 4 Purposes of Dreams</td>\n",
       "      <td>Passionate about the synergy between science a...</td>\n",
       "      <td>https://medium.com/science-for-real/the-4-purp...</td>\n",
       "      <td>['Eshan Samaranayake']</td>\n",
       "      <td>2020-12-21 16:05:19.524000+00:00</td>\n",
       "      <td>['Health', 'Neuroscience', 'Mental Health', 'P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Surviving a Rod Through the Head</td>\n",
       "      <td>You’ve heard of him, haven’t you? Phineas Gage...</td>\n",
       "      <td>https://medium.com/live-your-life-on-purpose/s...</td>\n",
       "      <td>['Rishav Sinha']</td>\n",
       "      <td>2020-02-26 00:01:01.576000+00:00</td>\n",
       "      <td>['Brain', 'Health', 'Development', 'Psychology...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title  \\\n",
       "0               Mental Note Vol. 24   \n",
       "1         Your Brain On Coronavirus   \n",
       "2                    Mind Your Nose   \n",
       "3          The 4 Purposes of Dreams   \n",
       "4  Surviving a Rod Through the Head   \n",
       "\n",
       "                                                text  \\\n",
       "0  Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...   \n",
       "1  Your Brain On Coronavirus\\n\\nA guide to the cu...   \n",
       "2  Mind Your Nose\\n\\nHow smell training can chang...   \n",
       "3  Passionate about the synergy between science a...   \n",
       "4  You’ve heard of him, haven’t you? Phineas Gage...   \n",
       "\n",
       "                                                 url                 authors  \\\n",
       "0  https://medium.com/invisible-illness/mental-no...            ['Ryan Fan']   \n",
       "1  https://medium.com/age-of-awareness/how-the-pa...       ['Simon Spichak']   \n",
       "2  https://medium.com/neodotlife/mind-your-nose-f...                      []   \n",
       "3  https://medium.com/science-for-real/the-4-purp...  ['Eshan Samaranayake']   \n",
       "4  https://medium.com/live-your-life-on-purpose/s...        ['Rishav Sinha']   \n",
       "\n",
       "                          timestamp  \\\n",
       "0  2020-12-26 03:38:10.479000+00:00   \n",
       "1  2020-09-23 22:10:17.126000+00:00   \n",
       "2  2020-10-10 20:17:37.132000+00:00   \n",
       "3  2020-12-21 16:05:19.524000+00:00   \n",
       "4  2020-02-26 00:01:01.576000+00:00   \n",
       "\n",
       "                                                tags  \n",
       "0  ['Mental Health', 'Health', 'Psychology', 'Sci...  \n",
       "1  ['Mental Health', 'Coronavirus', 'Science', 'P...  \n",
       "2  ['Biotechnology', 'Neuroscience', 'Brain', 'We...  \n",
       "3  ['Health', 'Neuroscience', 'Mental Health', 'P...  \n",
       "4  ['Brain', 'Health', 'Development', 'Psychology...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing, Text Preprocessing and Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is text Processing?\n",
    " - The term text processing refers to the automation of analyzing electronic text. This allows machine learning models to get structured information about the text to use for analysis, manipulation of the text, or to generate new text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is text Pre-processing?\n",
    " - transforming text into a clean and consistent format that can then be fed into a model for further analysis and learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://exchange.scale.com/public/blogs/preprocessing-techniques-in-nlp-a-guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.datarobot.com/blog/text-processing-what-why-and-how/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing broadly involes the following<br>\n",
    "<ol>\n",
    "    1. Lower case conversion <br>\n",
    "    2. Removal of special characteristics, digits, HTML tags, urls in the text <br>\n",
    "    3. Stop words removal - Stop words are those words that even after removal the meaning of the text remains same.<br>\n",
    "    4. Text Normalization {Root word conversion} - also known as stemming or lemmatization<br>\n",
    "    5. Spell correction <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '''To replace those goofy quantities with the far less retarded metric system (even though liters are considered part of the metric system they are the same as cubic deci-meters) you would say 770 cubic meters of kerosene {abbreviated as m3} and 1,204 m3 of liquid O2 [O2 is the symbol for oxygen]. We would also say it produced 33,600,000 newtons of force <abbreviated as N>.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To replace those goofy quantities with the far less retarded metric system (even though liters are considered part of the metric system they are the same as cubic deci-meters) you would say 770 cubic meters of kerosene {abbreviated as m3} and 1,204 m3 of liquid O2 [O2 is the symbol for oxygen]. We would also say it produced 33,600,000 newtons of force <abbreviated as N>.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Lower conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to replace those goofy quantities with the far less retarded metric system (even though liters are considered part of the metric system they are the same as cubic deci-meters) you would say 770 cubic meters of kerosene {abbreviated as m3} and 1,204 m3 of liquid o2 [o2 is the symbol for oxygen]. we would also say it produced 33,600,000 newtons of force <abbreviated as n>.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Removal of special characteristics, digits, HTML tags, urls in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[^a-zA-Z\\\\s]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_digits = True\n",
    "pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To replace those goofy quantities with the far less retarded metric system even though liters are considered part of the metric system they are the same as cubic deci meters you would say cubic meters of kerosene abbreviated as m and m of liquid O O is the symbol for oxygen We would also say it produced newtons of force abbreviated as N '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repl_s = re.sub(pattern, ' ', s)\n",
    "re.sub(r'\\s+', ' ', repl_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Stop words removal - Stop words are those words that even after removal the meaning of the text remains same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sridhar\n",
      "[nltk_data]     Kamoji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To replace goofy quantities far less retarded metric system (even though liters considered part metric system cubic deci-meters) would say 770 cubic meters kerosene {abbreviated m3} 1,204 m3 liquid O2 [O2 symbol oxygen]. We would also say produced 33,600,000 newtons force <abbreviated N>.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = s.split(' ')\n",
    "words = [word for word in words if word not in stop_words]\n",
    "repl_s = ' '.join(words)\n",
    "repl_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Root word conversion - also known as stemming or lemmatization (both are different)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization are text normalization techniques and are a part of text preprocessing steps <br><br>\n",
    "1. Stemming is the process of producing morphological variants of a root/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers. <br>\n",
    "2. lemmatization looks beyond word reduction and considers a language’s full vocabulary to apply a morphological analysis to words. <br>\n",
    "\n",
    "ref: https://towardsdatascience.com/stemming-vs-lemmatization-2daddabcb221 <br>\n",
    "\n",
    "ref: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n",
      "saw --> saw\n",
      "see --> see\n",
      "visit --> visit\n",
      "visiting --> visit\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "words = ['run','runner','running','ran','runs','easily','fairly', 'saw', 'see', 'visit', 'visiting']\n",
    "for word in words:\n",
    "    print(word+' --> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"I saw eighteen mice today!\")\n",
    "# doc = nlp(u\"I have seen my cats playing in sand!\")\n",
    "# doc = nlp(u\"I had to see my cat chasing the ball like a monkey!\")\n",
    "doc = nlp(u\"I love visiting New York!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(I, 'I'),\n",
       " (love, 'love'),\n",
       " (visiting, 'visit'),\n",
       " (New, 'New'),\n",
       " (York, 'York'),\n",
       " (!, '!')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token, token.lemma_) for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Spell Correction [Homework]<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ref: https://pypi.org/project/pyspellchecker/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happening\n",
      "{'penning', 'henning', 'happening'}\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# find those words that may be misspelled\n",
    "misspelled = spell.unknown(['something', 'is', 'hapenning', 'here'])\n",
    "\n",
    "for word in misspelled:\n",
    "    # Get the one `most likely` answer\n",
    "    print(spell.correction(word))\n",
    "\n",
    "    # Get a list of `likely` options\n",
    "    print(spell.candidates(word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
