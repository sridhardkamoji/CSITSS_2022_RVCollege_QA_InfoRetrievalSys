{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing and Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Text Preprocessing code\n",
    "2. BOW and TFIDF Representation\n",
    "3. Working Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen \n",
    "from urllib.parse import urlparse\n",
    "import numpy as np\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "import spacy\n",
    "import random\n",
    "import datetime\n",
    "import ssl\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "import itertools\n",
    "\n",
    "import spacy\n",
    "import pandas as pd \n",
    "import gensim\n",
    "import ast\n",
    "import datasets\n",
    "\n",
    "import os\n",
    "import wordcloud\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams \n",
    "from bs4 import BeautifulSoup , NavigableString\n",
    "import re \n",
    "import datetime \n",
    "import random\n",
    "import pickle\n",
    "import string\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "import string\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extract_stopwords():\n",
    "    ''' this class is used to extract a list of long stop words list from \n",
    "        https://www.ranks.nl/stopwords \n",
    "        Thanks to these ppl for providing this list'''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def extract_stopwords(self):\n",
    "        # Extracting long stop words\n",
    "        user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36'\n",
    "        \n",
    "        headers={'User-Agent':user_agent} \n",
    "        cont = ssl._create_unverified_context()\n",
    "\n",
    "        url = r'https://www.ranks.nl/stopwords'\n",
    "        request=urllib.request.Request(url, None, headers)\n",
    "        response = urllib.request.urlopen(request, context=cont)\n",
    "\n",
    "        data = response.read()\n",
    "        sw_soup = BeautifulSoup(data)\n",
    "        tbls = sw_soup.find_all('table')\n",
    "        req_tbl = tbls[3]\n",
    "\n",
    "        for i, item in enumerate(req_tbl.findAll('tr')):\n",
    "            stp_wd = [item.findAll('td')[i].findAll(text=True) for i in range(len(item.findAll('td')))]\n",
    "\n",
    "        stop_words = list(itertools.chain.from_iterable(stp_wd))\n",
    "        stop_words = [x.lower() for x in stop_words]\n",
    "        return stop_words\n",
    "    \n",
    "    \n",
    "class Preprocess_text():\n",
    "    '''Used to preprocess text.\n",
    "       Just pass True/False as the parameter to the operation you want to perform on the text and \n",
    "       call the preprocess_text() function by passing the text as argument. \n",
    "    '''\n",
    "    def __init__(self, special_char_rem= True, rem_urls = True, rem_digits = True, spell_correction = False,\n",
    "                 l_case = False, stop_words_rem = True, stemming=False, lemmatization_ = False,  rem_html_tags = True):\n",
    "        \n",
    "        self.special_char_rem = special_char_rem\n",
    "        self.rem_digits = rem_digits\n",
    "        self.l_case = l_case\n",
    "        self.stop_words_rem = stop_words_rem\n",
    "        self.stemming = stemming\n",
    "        self.lemmatization_ = lemmatization_\n",
    "        self.spell_correction = spell_correction\n",
    "        self.rem_urls = rem_urls\n",
    "        self.rem_html_tags = rem_html_tags\n",
    "        \n",
    "        if (self.lemmatization_ and self.stemming):\n",
    "            # priority given to lemmatization\n",
    "            self.stemming = False\n",
    "        \n",
    "        if self.lemmatization_:\n",
    "            self.stemming = False\n",
    "            \n",
    "        elif self.stemming:\n",
    "            self.lemmatization_ = False\n",
    "        \n",
    "            \n",
    "        if self.lemmatization_:\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        if self.stemming:\n",
    "            self.ps = nltk.porter.PorterStemmer()\n",
    "        \n",
    "        if self.stop_words_rem:\n",
    "            try:\n",
    "                self.stop_words = pickle.load(open('./long_stop_words_list.pkl', 'rb'))\n",
    "            except:\n",
    "                try:\n",
    "                    self.stop_words = extract_stopwords().extract_stopwords()\n",
    "                    pickle.dump(self.stop_words, open('./long_stop_words_list.pkl', 'wb'))\n",
    "                except:\n",
    "                    self.stop_words = stopwords('english')\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "    def remove_urls(self, text):\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        [s.extract() for s in soup(['iframe', 'script'])]\n",
    "        stripped_text = soup.get_text()\n",
    "        stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "        return stripped_text\n",
    "    \n",
    "    def remove_html_tags(self, text):\n",
    "        text = re.sub(r'<.*?>', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "    \n",
    "    def simple_stemming(self, text):\n",
    "        stemmer=self.ps\n",
    "        text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "        return text\n",
    "    \n",
    "    def simple_lemma(self, text, pos_tags = ['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        doc = self.nlp(text)\n",
    "        lemm_txt = [token.lemma_ for token in doc if token.pos_ in pos_tags]\n",
    "        lemm_txt = ' '.join(lemm_txt)\n",
    "        return lemm_txt\n",
    "    \n",
    "\n",
    "    def lower_case(self, text):\n",
    "        text = ' '.join([token.lower() for token in text.split()])\n",
    "        return text\n",
    "\n",
    "    def remove_special_characters(self, text, remove_digits= True):\n",
    "        pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "        text = re.sub(pattern, ' ', text)\n",
    "        text = re.sub('\\s+', ' ', text)\n",
    "        return text\n",
    "    \n",
    "    def spell_corrector(self, text):\n",
    "        spell = SpellChecker()\n",
    "        tokens = text.split()\n",
    "        tokens = [token.strip() for token in tokens]\n",
    "        corrected_tokens = [spell.correction(token) for token in tokens]\n",
    "        text = ' '.join(corrected_tokens)\n",
    "        return text\n",
    "\n",
    "    def remove_stopwords(self, text, is_lower_case=False):\n",
    "        stopwords = self.stop_words\n",
    "        tokens = text.split()\n",
    "    #     tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        tokens = [token.strip() for token in tokens]\n",
    "\n",
    "        if is_lower_case:\n",
    "            filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "        else:\n",
    "            filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "\n",
    "        filtered_text = ' '.join(filtered_tokens)    \n",
    "        return filtered_text\n",
    "    \n",
    "    def preprocess_text(self, text):  \n",
    "        \n",
    "        if self.rem_urls:\n",
    "            text = self.remove_urls(text)\n",
    "\n",
    "        if self.rem_html_tags:\n",
    "            text = self.remove_html_tags(text)\n",
    "            \n",
    "        if self.special_char_rem:\n",
    "            text = self.remove_special_characters(text, remove_digits=self.rem_digits)\n",
    "            \n",
    "            \n",
    "        if self.spell_correction:\n",
    "            text = self.spell_corrector(text)\n",
    "\n",
    "        if self.l_case:\n",
    "            text = self.lower_case(text)\n",
    "\n",
    "        if self.stop_words_rem:\n",
    "            text = self.remove_stopwords(text)\n",
    "\n",
    "        if self.stemming:\n",
    "            text = self.simple_stemming(text)\n",
    "            \n",
    "        if self.lemmatization_:\n",
    "            text = self.simple_lemma(text)\n",
    "            \n",
    "        self.text = text\n",
    "\n",
    "        return self.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration fabiochiu--medium-articles-96791ff68926910d\n",
      "Reusing dataset csv (C:\\Users\\Sridhar Kamoji\\.cache\\huggingface\\datasets\\fabiochiu___csv\\fabiochiu--medium-articles-96791ff68926910d\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240e6a8e349f40e683063c684c101825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset Ref:\n",
    "# https://huggingface.co/datasets/fabiochiu/medium-articles\n",
    "data = datasets.load_dataset('fabiochiu/medium-articles', data_files= 'medium_articles.csv')\n",
    "\n",
    "# converting the dataset_dictionary object to pandas dataframe\n",
    "data = pd.DataFrame.from_dict(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(n = 1000, random_state=32)\n",
    "data.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization of Text Preprocessing Object\n",
    "pp_text = Preprocess_text(special_char_rem=True,\n",
    "    rem_urls=True,\n",
    "    rem_digits=True,\n",
    "    spell_correction=False,\n",
    "    l_case=True,\n",
    "    stop_words_rem=True,\n",
    "    stemming=False,\n",
    "    lemmatization_=True,\n",
    "    rem_html_tags=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*HEALTY COACH*'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'healty coach'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title'][500]\n",
    "pp_text.preprocess_text(data['title'][500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the title\n",
    "\n",
    "data['pp_title'] = [pp_text.preprocess_text(txt) for txt in data['title']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words and TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer() # Bow\n",
    "tvec = TfidfVectorizer() # TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec.fit(data['pp_title'][:5].tolist())\n",
    "tvec.fit(data['pp_title'][:5].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_data = cvec.transform(data['pp_title'][:5].tolist())\n",
    "tvec_data = tvec.transform(data['pp_title'][:5].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>deutsch</th>\n",
       "      <th>improve</th>\n",
       "      <th>management</th>\n",
       "      <th>pain</th>\n",
       "      <th>phone</th>\n",
       "      <th>risk</th>\n",
       "      <th>strong</th>\n",
       "      <th>struggle</th>\n",
       "      <th>write</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>case write phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>struggle strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>improve risk management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>deutsch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   case  deutsch  improve  management  pain  phone  risk  strong  struggle  \\\n",
       "0     1        0        0           0     0      1     0       0         0   \n",
       "1     0        0        0           0     0      0     0       1         1   \n",
       "2     0        0        1           1     0      0     1       0         0   \n",
       "3     0        1        0           0     0      0     0       0         0   \n",
       "4     0        0        0           0     1      0     0       0         0   \n",
       "\n",
       "   write                     text  \n",
       "0      1         case write phone  \n",
       "1      0          struggle strong  \n",
       "2      0  improve risk management  \n",
       "3      0                  deutsch  \n",
       "4      0                     pain  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec_data = pd.DataFrame(cvec_data.toarray(), columns= cvec.get_feature_names())\n",
    "cvec_data['text'] = data['pp_title'][:5].tolist()\n",
    "cvec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('case', 2.09861228866811),\n",
       " ('deutsch', 2.09861228866811),\n",
       " ('improve', 2.09861228866811),\n",
       " ('management', 2.09861228866811),\n",
       " ('pain', 2.09861228866811),\n",
       " ('phone', 2.09861228866811),\n",
       " ('risk', 2.09861228866811),\n",
       " ('strong', 2.09861228866811),\n",
       " ('struggle', 2.09861228866811),\n",
       " ('write', 2.09861228866811)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(tvec.get_feature_names(), tvec.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>deutsch</th>\n",
       "      <th>improve</th>\n",
       "      <th>management</th>\n",
       "      <th>pain</th>\n",
       "      <th>phone</th>\n",
       "      <th>risk</th>\n",
       "      <th>strong</th>\n",
       "      <th>struggle</th>\n",
       "      <th>write</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>case write phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>struggle strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>improve risk management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>deutsch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>pain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      case  deutsch  improve  management  pain    phone     risk    strong  \\\n",
       "0  0.57735      0.0  0.00000     0.00000   0.0  0.57735  0.00000  0.000000   \n",
       "1  0.00000      0.0  0.00000     0.00000   0.0  0.00000  0.00000  0.707107   \n",
       "2  0.00000      0.0  0.57735     0.57735   0.0  0.00000  0.57735  0.000000   \n",
       "3  0.00000      1.0  0.00000     0.00000   0.0  0.00000  0.00000  0.000000   \n",
       "4  0.00000      0.0  0.00000     0.00000   1.0  0.00000  0.00000  0.000000   \n",
       "\n",
       "   struggle    write                     text  \n",
       "0  0.000000  0.57735         case write phone  \n",
       "1  0.707107  0.00000          struggle strong  \n",
       "2  0.000000  0.00000  improve risk management  \n",
       "3  0.000000  0.00000                  deutsch  \n",
       "4  0.000000  0.00000                     pain  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvec_data = pd.DataFrame(tvec_data.toarray(), columns= tvec.get_feature_names())\n",
    "tvec_data['text'] = data['pp_title'][:5].tolist()\n",
    "tvec_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Session (Audience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sentence Tokenize the text column\n",
    "2. Preprocess the text column\n",
    "3. Represent the data using BOW and Tfidf\n",
    "4. Experiment with ngram_range, min_df, max_df etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
